{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dir(os)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Jan  3 12:20:26 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 460.91.03    Driver Version: 460.91.03    CUDA Version: 11.2     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla M60           Off  | 00000000:00:0B.0 Off |                  Off |\n",
      "| N/A   35C    P0    38W / 150W |   2186MiB /  8129MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  Tesla M60           Off  | 00000000:00:0C.0 Off |                  Off |\n",
      "| N/A   29C    P8    14W / 150W |      3MiB /  8129MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A     12553      C   ...3/envs/testenv/bin/python      719MiB |\n",
      "|    0   N/A  N/A     16502      C   ...3/envs/testenv/bin/python      739MiB |\n",
      "|    0   N/A  N/A     21886      C   ...3/envs/testenv/bin/python      721MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "# sys.argv = ['']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# python3 run_compare_clust.py -ts=10 -e=20 -taq=50 -nq=40 -al_it=9 -init_ts=20 -pss=300 -its=33 -cl_s=35 -aq_s=bald -path_s=final_paper_experiments -exp_n=ts_10_e_20_taq_50_pss_300_its_33_norm_bald_mc_60_clust -clus -mc_d -tr_m\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# swcs results\n",
    "# ag_news_f1 = 0.9261822295409751\n",
    "# cola_f1 = 0.8199285328837826\n",
    "# subjectivity_f1 = 0.9694995191478745\n",
    "# polarity_f1 = 0.8907653824054689"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "av_data_path = './available_datasets'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_names = os.listdir(av_data_path)\n",
    "\n",
    "dataset_pathss = [os.path.join(av_data_path,i) for i in dataset_names]\n",
    "dataset_pathss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_csv_path = [os.path.join(i,'train.csv') for i in dataset_pathss]\n",
    "\n",
    "datafrmae_sizes = [len(pd.read_csv(i)) for i in train_csv_path]\n",
    "\n",
    "datafrmae_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normal results\n",
    "ag_news_f1_top = 0.9212030699758819\n",
    "cola_f1_top = 0.8088000431184998\n",
    "subjectivity_f1_top = 0.9700009008107298\n",
    "polarity_f1_top = 0.886988753263728"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_performances = {}\n",
    "\n",
    "top_performances['ag_news'] = ag_news_f1_top\n",
    "top_performances['cola'] = cola_f1_top\n",
    "top_performances['subjectivity'] = subjectivity_f1_top\n",
    "top_performances['polarity'] = polarity_f1_top"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_performances = {'ag_news': 0.9212030699758819,\n",
    " 'cola': 0.8088000431184998,\n",
    " 'subjectivity': 0.9700009008107298,\n",
    " 'polarity': 0.886988753263728}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_performances_80_percent = {i:top_performances[i]*0.9 for i in top_performances}\n",
    "top_performances_80_percent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_performances_80_percent = {i:top_performances[i]*0.95 for i in top_performances}\n",
    "top_performances_80_percent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls available_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[int(i*0.05) for i in datafrmae_sizes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.argv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.argv = [''] + ['-aq_s=bald','-path_s=final_paper_experiments','-exp_n=temp_ts_10_e_20_taq_50_nq_40_init_ts_100_pss_300_its_33_norm_bald_mc_60_clust']\n",
    "# sys.argv += ['-nq=40','-ts=10','-init_ts=100','-clus', '-badv', '-swag','-data=polarity','-cl_m=high', '-low_r']\n",
    "# sys.argv += ['-nq=40','-ts=10','-init_ts=100','-pss=0.05','-clus','-data=ag_news','-cl_m=high']\n",
    "sys.argv += ['-nq=40','-ts=10','-init_ts=20','-pss=0.05','-data=ag_news','-cl_m=high','-clus']\n",
    "# sys.argv.remove('-f')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ff = \"-ts=2 -e=20 -taq=50 -nq=10 -al_it=9 -init_ts=20 -pss=0.05 -its=17 -cl_s=17 -aq_s=bald -path_s=final_publish_paper_experiments_v2 -exp_n=subjectivity_ts_10_e_20_taq_50_nq_10_init_ts_20_pss_05_its_17_swca_bald_swag_sampling_high_clust -data=subjectivity -clus -cl_m=high -badv -swag\"\n",
    "\n",
    "sys.argv = ['']\n",
    "sys.argv += ff.split(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.argv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "['ag_news','cola','polarity','subjectivity']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/gpfs/cfms/home/cconstantinou/miniconda3/envs/packclone/lib/python3.6/site-packages/ipykernel_launcher.py',\n",
       " '-f',\n",
       " '/gpfs/cfms/home/cconstantinou/.local/share/jupyter/runtime/kernel-a82e559a-711f-4e21-81ea-3cddc7a24a72.json']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sys.argv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['-ts=10',\n",
       " '-e=20',\n",
       " '-taq=50',\n",
       " '-nq=10',\n",
       " '-al_it=13',\n",
       " '-init_ts=20',\n",
       " '-pss=0.05',\n",
       " '-its=30',\n",
       " '-cl_s=16',\n",
       " '-aq_s=bald',\n",
       " '-path_s=final_paper_experiments',\n",
       " '-exp_n=temp_ts_10_e_20_taq_50_nq_40_init_ts_100_pss_300_its_33_norm_bald_mc_60_clust',\n",
       " '-data=cola',\n",
       " '-clus',\n",
       " '-cl_m=high',\n",
       " '-mc_d',\n",
       " '-tr_m']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# jk = '-ts=10 -e=20 -taq=50 -nq=10 -al_it=13 -init_ts=20 -pss=0.05 -its=30 -cl_s=16 -aq_s=bald -path_s=final_paper_experiments -exp_n=temp_ts_10_e_20_taq_50_nq_40_init_ts_100_pss_300_its_33_norm_bald_mc_60_clust -data=cola -clus -cl_m=high -badv -swag'\n",
    "jk = '-ts=10 -e=20 -taq=50 -nq=10 -al_it=13 -init_ts=20 -pss=0.05 -its=30 -cl_s=16 -aq_s=bald -path_s=final_paper_experiments -exp_n=temp_ts_10_e_20_taq_50_nq_40_init_ts_100_pss_300_its_33_norm_bald_mc_60_clust -data=cola -clus -cl_m=high -mc_d -tr_m'\n",
    "\n",
    "jk.split(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.argv = [''] + jk.split(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " '-ts=10',\n",
       " '-e=20',\n",
       " '-taq=50',\n",
       " '-nq=10',\n",
       " '-al_it=13',\n",
       " '-init_ts=20',\n",
       " '-pss=0.05',\n",
       " '-its=30',\n",
       " '-cl_s=16',\n",
       " '-aq_s=bald',\n",
       " '-path_s=final_paper_experiments',\n",
       " '-exp_n=temp_ts_10_e_20_taq_50_nq_40_init_ts_100_pss_300_its_33_norm_bald_mc_60_clust',\n",
       " '-data=cola',\n",
       " '-clus',\n",
       " '-cl_m=high',\n",
       " '-mc_d',\n",
       " '-tr_m']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sys.argv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "{'trials_strategy': 10, 'n_epochs_train': 20, 'T_aq_uncert': 50, 'n_query_pick': 10, 'active_learning_iters': 13, 'initial_train_data_size': 20, 'percentage_sample_size_pool_inference': 0.05, 'initial_train_seed': 30, 'cluster_seed': 16, 'aquisition_trategy': 'bald', 'path_save': PosixPath('final_paper_experiments'), 'dataset_name': 'cola', 'clustering_strategy': 'high', 'experiment_name': 'temp_ts_10_e_20_taq_50_nq_40_init_ts_100_pss_300_its_33_norm_bald_mc_60_clust', 'bays_adv_approach': False, 'use_swag': False, 'use_clustering': True, 'train_mode': True, 'mc_dropout': True, 'low_rank': False}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "01/03/2022 12:21:08 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True\n",
      "01/03/2022 12:21:08 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_find_unused_parameters=None,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=True,\n",
      "do_predict=False,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_steps=None,\n",
      "evaluation_strategy=IntervalStrategy.NO,\n",
      "fp16=True,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "gradient_accumulation_steps=2,\n",
      "gradient_checkpointing=False,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "hub_model_id=None,\n",
      "hub_strategy=HubStrategy.EVERY_SAVE,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=5e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=-1,\n",
      "log_level=-1,\n",
      "log_level_replica=-1,\n",
      "log_on_each_node=True,\n",
      "logging_dir=./experiment_run_classification/runs/Jan03_12-21-08_dsserver02.int.cfms.org.uk,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=100,\n",
      "logging_strategy=IntervalStrategy.STEPS,\n",
      "lr_scheduler_type=SchedulerType.LINEAR,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=20,\n",
      "output_dir=./experiment_run_classification/,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=32,\n",
      "per_device_train_batch_size=32,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "remove_unused_columns=True,\n",
      "report_to=['tensorboard'],\n",
      "resume_from_checkpoint=False,\n",
      "run_name=./experiment_run_classification/,\n",
      "save_on_each_node=False,\n",
      "save_steps=5000,\n",
      "save_strategy=IntervalStrategy.STEPS,\n",
      "save_total_limit=7,\n",
      "seed=42,\n",
      "sharded_ddp=[],\n",
      "skip_memory_metrics=True,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_legacy_prediction_loop=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      "xpu_backend=None,\n",
      ")\n",
      "01/03/2022 12:21:32 - WARNING - datasets.builder -   Using custom data configuration default-5e9177251dbd218a\n",
      "01/03/2022 12:21:32 - WARNING - datasets.builder -   Reusing dataset csv (/gpfs/cfms/home/cconstantinou/.cache/huggingface/datasets/csv/default-5e9177251dbd218a/0.0.0/9144e0a4e8435090117cea53e6c7537173ef2304525df4a077c435d8ee7828ff)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fcbbca4b29164ed38e928abb4646d3b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|configuration_utils.py:583] 2022-01-03 12:21:33,201 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /gpfs/cfms/home/cconstantinou/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "/gpfs/cfms/home/cconstantinou/miniconda3/envs/packclone/lib/python3.6/site-packages/transformers/configuration_utils.py:337: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  \"Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 \"\n",
      "[INFO|configuration_utils.py:620] 2022-01-03 12:21:33,204 >> Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.11.0.dev0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:1741] 2022-01-03 12:21:35,398 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /gpfs/cfms/home/cconstantinou/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
      "[INFO|tokenization_utils_base.py:1741] 2022-01-03 12:21:35,400 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /gpfs/cfms/home/cconstantinou/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n",
      "[INFO|tokenization_utils_base.py:1741] 2022-01-03 12:21:35,401 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1741] 2022-01-03 12:21:35,402 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1741] 2022-01-03 12:21:35,402 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /gpfs/cfms/home/cconstantinou/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n",
      "[INFO|configuration_utils.py:583] 2022-01-03 12:21:35,767 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /gpfs/cfms/home/cconstantinou/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "[INFO|configuration_utils.py:620] 2022-01-03 12:21:35,769 >> Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.11.0.dev0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "[INFO|configuration_utils.py:583] 2022-01-03 12:21:36,188 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /gpfs/cfms/home/cconstantinou/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "[INFO|configuration_utils.py:620] 2022-01-03 12:21:36,190 >> Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.11.0.dev0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "[INFO|modeling_utils.py:1323] 2022-01-03 12:21:36,564 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /gpfs/cfms/home/cconstantinou/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n",
      "[WARNING|modeling_utils.py:1580] 2022-01-03 12:21:40,567 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification_c: ['cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification_c from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification_c from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "[WARNING|modeling_utils.py:1591] 2022-01-03 12:21:40,569 >> Some weights of BertForSequenceClassification_c were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "[INFO|modeling_utils.py:1323] 2022-01-03 12:21:40,950 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /gpfs/cfms/home/cconstantinou/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n",
      "[WARNING|modeling_utils.py:1580] 2022-01-03 12:21:43,866 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification_c: ['cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification_c from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification_c from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "[WARNING|modeling_utils.py:1591] 2022-01-03 12:21:43,867 >> Some weights of BertForSequenceClassification_c were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "01/03/2022 12:21:44 - WARNING - datasets.arrow_dataset -   Loading cached processed dataset at /gpfs/cfms/home/cconstantinou/.cache/huggingface/datasets/csv/default-5e9177251dbd218a/0.0.0/9144e0a4e8435090117cea53e6c7537173ef2304525df4a077c435d8ee7828ff/cache-409e6acc6a148a1c.arrow\n",
      "01/03/2022 12:21:44 - WARNING - datasets.arrow_dataset -   Loading cached processed dataset at /gpfs/cfms/home/cconstantinou/.cache/huggingface/datasets/csv/default-5e9177251dbd218a/0.0.0/9144e0a4e8435090117cea53e6c7537173ef2304525df4a077c435d8ee7828ff/cache-a941b4a286b36f1e.arrow\n",
      "01/03/2022 12:21:44 - WARNING - datasets.arrow_dataset -   Loading cached processed dataset at /gpfs/cfms/home/cconstantinou/.cache/huggingface/datasets/csv/default-5e9177251dbd218a/0.0.0/9144e0a4e8435090117cea53e6c7537173ef2304525df4a077c435d8ee7828ff/cache-d9ceeb9ad90d9243.arrow\n",
      "01/03/2022 12:21:44 - INFO - __main__ -   Sample 1337 of the training set: {'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'input_ids': [101, 2016, 2001, 2741, 1037, 3319, 6100, 1997, 1996, 2338, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': 1, 'text': 'She was sent a review copy of the book.', 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
      "01/03/2022 12:21:44 - INFO - __main__ -   Sample 6935 of the training set: {'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'input_ids': [101, 1996, 7427, 5225, 2000, 2047, 2259, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': 0, 'text': 'The package drove to New York.', 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
      "01/03/2022 12:21:44 - INFO - __main__ -   Sample 3282 of the training set: {'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'input_ids': [101, 1996, 16669, 2596, 1037, 10855, 2013, 2010, 10353, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': 0, 'text': 'The magician appeared a dove from his sleeve.', 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n"
     ]
    }
   ],
   "source": [
    "from bert_trainer_utils import *\n",
    "from arg_config import arguments\n",
    "import json\n",
    "import re\n",
    "import argparse\n",
    "import pathlib\n",
    "\n",
    "from bert_modeling_custom import BertForSequenceClassification_c\n",
    "from bert_classifier_model import Bert_classifier_adversarial\n",
    "from swag_modeling import SWAG_adversarial\n",
    "\n",
    "from active_learning import Active_learner\n",
    "from data_handler import Data_Handler\n",
    "from aquisition import aquisition_function,ALL_AQUISITIONS\n",
    "from al_params import params_active_l_n,params_active_l_b\n",
    "from clustering import get_clustered_indexes_dict\n",
    "\n",
    "task_to_keys = {\n",
    "    \"cola\": (\"sentence\", None),\n",
    "    \"mnli\": (\"premise\", \"hypothesis\"),\n",
    "    \"mrpc\": (\"sentence1\", \"sentence2\"),\n",
    "    \"qnli\": (\"question\", \"sentence\"),\n",
    "    \"qqp\": (\"question1\", \"question2\"),\n",
    "    \"rte\": (\"sentence1\", \"sentence2\"),\n",
    "    \"sst2\": (\"sentence\", None),\n",
    "    \"stsb\": (\"sentence1\", \"sentence2\"),\n",
    "    \"wnli\": (\"sentence1\", \"sentence2\"),\n",
    "}\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "data_sizes = {\"cola\":7592,\n",
    "              \"polarity\":7463,\n",
    "              \"subjectivity\":7000,\n",
    "              \"ag_news\":15000\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DataTrainingArguments:\n",
    "    \"\"\"\n",
    "    Arguments pertaining to what data we are going to input our model for training and eval.\n",
    "    Using `HfArgumentParser` we can turn this class\n",
    "    into argparse arguments to be able to specify them on\n",
    "    the command line.\n",
    "    \"\"\"\n",
    "\n",
    "    task_name: Optional[str] = field(\n",
    "        default=None,\n",
    "        metadata={\"help\": \"The name of the task to train on: \" + \", \".join(task_to_keys.keys())},\n",
    "    )\n",
    "    dataset_name: Optional[str] = field(\n",
    "        default=None, metadata={\"help\": \"The name of the dataset to use (via the datasets library).\"}\n",
    "    )\n",
    "    dataset_config_name: Optional[str] = field(\n",
    "        default=None, metadata={\"help\": \"The configuration name of the dataset to use (via the datasets library).\"}\n",
    "    )\n",
    "    max_seq_length: int = field(\n",
    "        default=128,\n",
    "        metadata={\n",
    "            \"help\": \"The maximum total input sequence length after tokenization. Sequences longer \"\n",
    "            \"than this will be truncated, sequences shorter will be padded.\"\n",
    "        },\n",
    "    )\n",
    "    overwrite_cache: bool = field(\n",
    "        default=False, metadata={\"help\": \"Overwrite the cached preprocessed datasets or not.\"}\n",
    "    )\n",
    "    pad_to_max_length: bool = field(\n",
    "        default=True,\n",
    "        metadata={\n",
    "            \"help\": \"Whether to pad all samples to `max_seq_length`. \"\n",
    "            \"If False, will pad the samples dynamically when batching to the maximum length in the batch.\"\n",
    "        },\n",
    "    )\n",
    "    max_train_samples: Optional[int] = field(\n",
    "        default=None,\n",
    "        metadata={\n",
    "            \"help\": \"For debugging purposes or quicker training, truncate the number of training examples to this \"\n",
    "            \"value if set.\"\n",
    "        },\n",
    "    )\n",
    "    max_eval_samples: Optional[int] = field(\n",
    "        default=None,\n",
    "        metadata={\n",
    "            \"help\": \"For debugging purposes or quicker training, truncate the number of evaluation examples to this \"\n",
    "            \"value if set.\"\n",
    "        },\n",
    "    )\n",
    "    max_predict_samples: Optional[int] = field(\n",
    "        default=None,\n",
    "        metadata={\n",
    "            \"help\": \"For debugging purposes or quicker training, truncate the number of prediction examples to this \"\n",
    "            \"value if set.\"\n",
    "        },\n",
    "    )\n",
    "    train_file: Optional[str] = field(\n",
    "        default=None, metadata={\"help\": \"A csv or a json file containing the training data.\"}\n",
    "    )\n",
    "    validation_file: Optional[str] = field(\n",
    "        default=None, metadata={\"help\": \"A csv or a json file containing the validation data.\"}\n",
    "    )\n",
    "    test_file: Optional[str] = field(\n",
    "        default=None, metadata={\"help\": \"A csv or a json file containing the test data.\"}\n",
    "    )\n",
    "        \n",
    "        \n",
    "    dataset_name_source: Optional[str] = field(\n",
    "        default=None, metadata={\"help\": \"a name to the training data folder.\"}\n",
    "    )\n",
    "    main_data_dir: Optional[str] = field(\n",
    "        default=None, metadata={\"help\": \"the main data directory consisting all datasets\"}\n",
    "    )\n",
    "        \n",
    "    use_adversarial: bool = field(\n",
    "        default=False, metadata={\"help\": \"whether to use adversarial training or not.\"}\n",
    "    )\n",
    "        \n",
    "    adversarial_epsilon: Optional[float] = field(\n",
    "        default=None,\n",
    "        metadata={\n",
    "            \"help\": \"parmeter used for adversarial attack\"\n",
    "        },\n",
    "    )\n",
    "        \n",
    "    lambdaa: Optional[float] = field(\n",
    "        default=None,\n",
    "        metadata={\n",
    "            \"help\": \"parameter that weights ths losses i.e constrastive loss and crossentropy losses\"\n",
    "        },\n",
    "    )\n",
    "        \n",
    "        \n",
    "    swag_per_start: Optional[float] = field(\n",
    "        default=None,\n",
    "        metadata={\n",
    "            \"help\": \"percentage of training to be done before starting swag\"\n",
    "        },\n",
    "    )\n",
    "        \n",
    "    temperature_contrastive: Optional[float] = field(\n",
    "        default=None,\n",
    "        metadata={\n",
    "            \"help\": \"use for scaling the output of cosine similirity between Z and Z_attacked\"\n",
    "        },\n",
    "    )\n",
    "        \n",
    "    adv_attk_tpe: Optional[str] = field(\n",
    "        default=None, metadata={\"help\": \"adversarial attack type\"}\n",
    "    )\n",
    "\n",
    "        \n",
    " \n",
    "\n",
    "    def __post_init__(self):\n",
    "        if self.task_name is not None:\n",
    "            self.task_name = self.task_name.lower()\n",
    "            if self.task_name not in task_to_keys.keys():\n",
    "                raise ValueError(\"Unknown task, you should pick one in \" + \",\".join(task_to_keys.keys()))\n",
    "        elif self.dataset_name is not None:\n",
    "            pass\n",
    "        elif self.train_file is None or self.validation_file is None:\n",
    "            raise ValueError(\"Need either a GLUE task, a training/validation file or a dataset name.\")\n",
    "        else:\n",
    "            train_extension = self.train_file.split(\".\")[-1]\n",
    "            assert train_extension in [\"csv\", \"json\"], \"`train_file` should be a csv or a json file.\"\n",
    "            validation_extension = self.validation_file.split(\".\")[-1]\n",
    "            assert (\n",
    "                validation_extension == train_extension\n",
    "            ), \"`validation_file` should have the same extension (csv or json) as `train_file`.\"\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ModelArguments:\n",
    "    \"\"\"\n",
    "    Arguments pertaining to which model/config/tokenizer we are going to fine-tune from.\n",
    "    \"\"\"\n",
    "\n",
    "    model_name_or_path: str = field(\n",
    "        metadata={\"help\": \"Path to pretrained model or model identifier from huggingface.co/models\"}\n",
    "    )\n",
    "    config_name: Optional[str] = field(\n",
    "        default=None, metadata={\"help\": \"Pretrained config name or path if not the same as model_name\"}\n",
    "    )\n",
    "    tokenizer_name: Optional[str] = field(\n",
    "        default=None, metadata={\"help\": \"Pretrained tokenizer name or path if not the same as model_name\"}\n",
    "    )\n",
    "    cache_dir: Optional[str] = field(\n",
    "        default=None,\n",
    "        metadata={\"help\": \"Where do you want to store the pretrained models downloaded from huggingface.co\"},\n",
    "    )\n",
    "    use_fast_tokenizer: bool = field(\n",
    "        default=True,\n",
    "        metadata={\"help\": \"Whether to use one of the fast tokenizer (backed by the tokenizers library) or not.\"},\n",
    "    )\n",
    "    model_revision: str = field(\n",
    "        default=\"main\",\n",
    "        metadata={\"help\": \"The specific model version to use (can be a branch name, tag name or commit id).\"},\n",
    "    )\n",
    "    use_auth_token: bool = field(\n",
    "        default=False,\n",
    "        metadata={\n",
    "            \"help\": \"Will use the token generated when running `transformers-cli login` (necessary to use this script \"\n",
    "            \"with private models).\"\n",
    "        },\n",
    "    )\n",
    "        \n",
    "def preprocess_function(examples):\n",
    "    # Tokenize the texts\n",
    "    args = (\n",
    "        (examples[sentence1_key],) if sentence2_key is None else (examples[sentence1_key], examples[sentence2_key])\n",
    "    )\n",
    "    result = tokenizer(*args, padding=padding, max_length=max_seq_length, truncation=True)\n",
    "\n",
    "    # Map labels to IDs (not necessary for GLUE tasks)\n",
    "    if label_to_id is not None and \"labels\" in examples:\n",
    "        result[\"labels\"] = [(label_to_id[l] if l != -1 else -1) for l in examples[\"labels\"]]\n",
    "    return result\n",
    "\n",
    "\n",
    "def compute_metrics(p: EvalPrediction):\n",
    "    preds = p.predictions[0] if isinstance(p.predictions, tuple) else p.predictions\n",
    "    preds = np.squeeze(preds) if is_regression else np.argmax(preds, axis=1)\n",
    "    if data_args.task_name is not None:\n",
    "        result = metric.compute(predictions=preds, references=p.label_ids)\n",
    "        if len(result) > 1:\n",
    "            result[\"combined_score\"] = np.mean(list(result.values())).item()\n",
    "        return result\n",
    "    elif is_regression:\n",
    "        return {\"mse\": ((preds - p.label_ids) ** 2).mean().item()}\n",
    "    else:\n",
    "        return {\"accuracy\": (preds == p.label_ids).astype(np.float32).mean().item()\n",
    "               }\n",
    "    \n",
    "def sample_data(train_dataset,n=100):\n",
    "    \n",
    "    \"\"\"\n",
    "    function to sample datasets\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    df_data = train_dataset.to_pandas()\n",
    "\n",
    "    df_data = df_data.sample(n=n,replace=False)\n",
    "\n",
    "    filter_df_indexes = df_data.index.values\n",
    "    \n",
    "    sample_train = train_dataset.select(list(filter_df_indexes))\n",
    "    \n",
    "    \n",
    "    return sample_train\n",
    "\n",
    "\n",
    "def generate_active_lr_results(trials_strategy=2,\n",
    "                               n_epochs_train=5,\n",
    "                               T_aq_uncert=5,\n",
    "                               n_query_pick=10,\n",
    "                               bays_adv_approach=False,\n",
    "                               active_learning_iters=2,\n",
    "                               initial_train_data_size=10,\n",
    "                               n_sample_size_pool_inference=250,\n",
    "                               use_swag=False,\n",
    "                               use_clustering=False,\n",
    "                               initial_train_seed=None,\n",
    "                               cluster_seed=None,\n",
    "                               mc_dropout=None,\n",
    "                               train_mode=None,\n",
    "                               cl_m=None,\n",
    "                               name_exp=None,\n",
    "                               low_rank=None,\n",
    "                               dataset_name=None\n",
    "                              ):\n",
    "    \n",
    "    \n",
    "    n_sample_size_pool_inference = int(data_sizes[dataset_name] * n_sample_size_pool_inference)\n",
    "    \n",
    "    print('n_sample_size_pool_inference:',n_sample_size_pool_inference)\n",
    "    \n",
    "    average_results = {i:{} for i in ALL_AQUISITIONS}\n",
    "    \n",
    "    if use_clustering:\n",
    "        indexs_per_cluster = get_clustered_indexes_dict(raw_datasets_init,\n",
    "                                                        rand_state=cluster_seed,\n",
    "                                                        cl_m=cl_m\n",
    "                                                       )\n",
    "    else:\n",
    "        indexs_per_cluster = None\n",
    "    \n",
    "    num_experiemnts = len(ALL_AQUISITIONS) * trials_strategy \n",
    "    n_done = 0\n",
    "    \n",
    "    \n",
    "    all_trials = {}\n",
    "    \n",
    "    assl_trinals_randoms = {}\n",
    "\n",
    "    for strategey in ALL_AQUISITIONS:\n",
    "\n",
    "        # matrices to average results\n",
    "        all_trials_ac_performace = np.zeros((trials_strategy,active_learning_iters)) \n",
    "        all_trials_b_ac_performace = np.zeros((trials_strategy,active_learning_iters)) \n",
    "        all_trials_f1_performace = np.zeros((trials_strategy,active_learning_iters)) \n",
    "\n",
    "\n",
    "        for trial in range(trials_strategy):\n",
    "            \n",
    "            assl_trinals_randoms[trial] = []\n",
    "            \n",
    "            # data handler\n",
    "            data_handler = Data_Handler(\n",
    "                train_dataset,\n",
    "                eval_dataset,\n",
    "                predict_dataset,\n",
    "                indexs_per_cluster\n",
    "            )\n",
    "\n",
    "\n",
    "            # epochs used for training\n",
    "            training_args.num_train_epochs = n_epochs_train\n",
    "\n",
    "            # active learner\n",
    "            active_model = Active_learner(\n",
    "                data_handler=data_handler,\n",
    "                data_args=data_args,\n",
    "                model_args=model_args,\n",
    "                training_args=training_args,\n",
    "                compute_metrics=compute_metrics,\n",
    "                tokenizer=tokenizer,\n",
    "                data_collator=data_collator,\n",
    "                criterion=criterion,\n",
    "                aquisiiton_f=aquisition_function,\n",
    "                num_labels=num_labels,\n",
    "                bays_aversarial=bays_adv_approach,\n",
    "                iters=active_learning_iters)\n",
    "\n",
    "            # run active learning experiment\n",
    "            active_model.active_train(\n",
    "                initial_n_train=initial_train_data_size,\n",
    "                n_sample_infer=n_sample_size_pool_inference,\n",
    "                T_aq=T_aq_uncert,\n",
    "                n_query=n_query_pick,\n",
    "                aquisition_strategy=strategey,\n",
    "                use_swag=use_swag,\n",
    "                r_s=trial,\n",
    "                initial_train_seed=initial_train_seed,\n",
    "                mc_dropout=mc_dropout,\n",
    "                train_mode=train_mode,\n",
    "                low_rank=low_rank\n",
    "            )\n",
    "\n",
    "\n",
    "            # trial performance\n",
    "            ac_performace_temp = active_model.test_performance_ac\n",
    "            b_ac_performace_temp = active_model.test_performance_b_ac\n",
    "            f1_performance = active_model.test_performance_f1\n",
    "            \n",
    "            \n",
    "            \n",
    "            rrrr = active_model.data_handler.all_random_ccc\n",
    "            \n",
    "            assl_trinals_randoms[trial].append(rrrr)\n",
    "\n",
    "            # number of train data\n",
    "            number_of_train_data = active_model.number_of_train_data\n",
    "\n",
    "            # adding metrics to matrix for averagind\n",
    "            all_trials_ac_performace[trial,:] = ac_performace_temp\n",
    "            all_trials_b_ac_performace[trial,:] = b_ac_performace_temp\n",
    "            all_trials_f1_performace[trial,:] = f1_performance\n",
    "            \n",
    "            n_done += 1\n",
    "            \n",
    "            print(f'EXPERIMENT PROGRES: {round(n_done/num_experiemnts*100)}% done ')\n",
    "            print(f'Experiment detials: \\n trials_strategy: {trials_strategy} \\n n_epochs_train: {n_epochs_train} \\n T_aq_uncert: {T_aq_uncert} \\n n_query_pick: {n_query_pick} \\n bays_adv_approach: {bays_adv_approach}\\n active_learning_iters: {active_learning_iters}\\n initial_train_data_size: {initial_train_data_size}\\n n_sample_size_pool_inference: {n_sample_size_pool_inference}\\n use_swag: {use_swag}\\n use_clustering: {use_clustering}\\n strategey: {strategey} \\n low_rank: {low_rank} \\n experiment nam: {name_exp}')\n",
    "                    \n",
    "        all_trials['acc'] = all_trials_ac_performace\n",
    "        all_trials['b_acc'] = all_trials_b_ac_performace\n",
    "        all_trials['f1'] = all_trials_f1_performace\n",
    "        # average results\n",
    "        average_results[strategey]['acc'] = all_trials_ac_performace.mean(axis=0).tolist()\n",
    "        average_results[strategey]['b_acc'] = all_trials_b_ac_performace.mean(axis=0).tolist()\n",
    "        average_results[strategey]['f1'] = all_trials_f1_performace.mean(axis=0).tolist()\n",
    "        average_results[strategey]['n_train'] = number_of_train_data\n",
    "    \n",
    "    return average_results\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    \n",
    "    parser = argparse.ArgumentParser()\n",
    "    \n",
    "    \n",
    "    \n",
    "    parser.add_argument(\"-ts\",\n",
    "                        \"--trials_strategy\",\n",
    "                        help=\"number of trials to perform on each strategy and then average\",\n",
    "                        type=int,\n",
    "                        default=10,\n",
    "                       )\n",
    "    \n",
    "    parser.add_argument(\"-e\",\n",
    "                        \"--n_epochs_train\",\n",
    "                        help=\"number of epochs to train model\",\n",
    "                        default=10,\n",
    "                        type=int,\n",
    "                       )\n",
    "    \n",
    "    parser.add_argument(\"-taq\",\n",
    "                        \"--T_aq_uncert\",\n",
    "                        help=\"number of times to infer each example\",\n",
    "                        type=int,\n",
    "                        default=50,\n",
    "                       )\n",
    "    \n",
    "    parser.add_argument(\"-nq\",\n",
    "                        \"--n_query_pick\",\n",
    "                        help=\"number of instances to add to train data after each\",\n",
    "                        type=int,\n",
    "                        default=10,\n",
    "                       )\n",
    "    \n",
    "    parser.add_argument(\"-al_it\",\n",
    "                        \"--active_learning_iters\",\n",
    "                        help=\"number of iterations to perform in the active learning cycle\",\n",
    "                        type=int,\n",
    "                        default=10,\n",
    "                       )\n",
    "    \n",
    "    parser.add_argument(\"-init_ts\",\n",
    "                        \"--initial_train_data_size\",\n",
    "                        help=\"number instances usd intialy for training\",\n",
    "                        type=int,\n",
    "                        default=20,\n",
    "                       )\n",
    "    \n",
    "#     parser.add_argument(\"-pss\",\n",
    "#                         \"--n_sample_size_pool_inference\",\n",
    "#                         help=\"number instances to sample from unlabelled pool\",\n",
    "#                         type=int,\n",
    "#                         default=400,\n",
    "#                        )\n",
    "    \n",
    "    parser.add_argument(\"-pss\",\n",
    "                        \"--percentage_sample_size_pool_inference\",\n",
    "                        help=\"number instances to sample from unlabelled pool\",\n",
    "                        type=float,\n",
    "                        default=0.05,\n",
    "                       )\n",
    "    \n",
    "    \n",
    "    parser.add_argument(\"-its\",\n",
    "                        \"--initial_train_seed\",\n",
    "                        help=\"initial seed to choose initial train instances \",\n",
    "                        type=int,\n",
    "                        default=74,\n",
    "                       )\n",
    "    \n",
    "    parser.add_argument(\"-cl_s\",\n",
    "                        \"--cluster_seed\",\n",
    "                        help=\"initial seed to form clusters \",\n",
    "                        type=int,\n",
    "                        default=73,\n",
    "                       )\n",
    "    \n",
    "    parser.add_argument(\"-aq_s\",\n",
    "                        \"--aquisition_trategy\",\n",
    "                        help=\"which aquisition strategy to use\",\n",
    "                        type=str,\n",
    "                        choices=['random','bald','var_ratios'],\n",
    "                        required=True\n",
    "                       )\n",
    "   \n",
    "    \n",
    "    parser.add_argument(\"-path_s\",\n",
    "                        \"--path_save\",\n",
    "                        help=\"path to save model\",\n",
    "                        type=pathlib.Path,\n",
    "                        required=True\n",
    "                       )\n",
    "    \n",
    "    parser.add_argument(\"-data\",\n",
    "                        \"--dataset_name\",\n",
    "                        help=\"dataset to use\",\n",
    "                        type=str,\n",
    "                        choices=['ag_news','cola','polarity','subjectivity','toxic'],\n",
    "                        required=True\n",
    "                       )\n",
    "    \n",
    "    \n",
    "    parser.add_argument(\"-cl_m\",\n",
    "                        \"--clustering_strategy\",\n",
    "                        help=\"dataset to use\",\n",
    "                        type=str,\n",
    "                        choices=['normal','high'],\n",
    "                        default='normal'\n",
    "                       )\n",
    "    \n",
    "\n",
    "    parser.add_argument(\"-exp_n\",\n",
    "                        \"--experiment_name\",\n",
    "                        help=\"wether to use swag method\",\n",
    "                        type=str,\n",
    "                        required=True\n",
    "                       )\n",
    "    \n",
    "    parser.add_argument(\"-badv\",\n",
    "                        \"--bays_adv_approach\",\n",
    "                        help=\"wether to use the baysian adversarial learning approach\",\n",
    "                        action=\"store_true\"\n",
    "                       )\n",
    "    \n",
    "    parser.add_argument(\"-swag\",\n",
    "                        \"--use_swag\",\n",
    "                        help=\"wether to use swag method\",\n",
    "                        action=\"store_true\"\n",
    "                       )\n",
    "    \n",
    "    parser.add_argument(\"-clus\",\n",
    "                        \"--use_clustering\",\n",
    "                        help=\"wether to use swag method\",\n",
    "                        action=\"store_true\"\n",
    "                       )\n",
    "    \n",
    "    parser.add_argument(\"-tr_m\",\n",
    "                        \"--train_mode\",\n",
    "                        help=\"wether to activate dropouts\",\n",
    "                        action=\"store_true\"\n",
    "                       )\n",
    "    \n",
    "    \n",
    "    parser.add_argument(\"-mc_d\",\n",
    "                        \"--mc_dropout\",\n",
    "                        help=\"wether use dropouts for sampling\",\n",
    "                        action=\"store_true\"\n",
    "                       )\n",
    "    \n",
    "    \n",
    "    parser.add_argument(\"-low_r\",\n",
    "                        \"--low_rank\",\n",
    "                        help=\"wether use low_rank covariance matrix for sampling\",\n",
    "                        action=\"store_true\"\n",
    "                       )\n",
    "    \n",
    "\n",
    "    args_al = parser.parse_args()\n",
    "    \n",
    "    \n",
    "    \n",
    "    print('\\n'*10)\n",
    "    print(args_al.__dict__)\n",
    "    print('\\n'*10)\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "    # create output folder if doesnt exist\n",
    "    output_dir = arguments['output_dir']\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "        \n",
    "    # write a temporary json file\n",
    "    json_file_to_parse = os.path.join(output_dir,\"temp_args.json\")\n",
    "    with open(json_file_to_parse, 'w') as f:\n",
    "        json.dump(arguments, f)\n",
    "\n",
    "    # pass arguments to respective argument classes\n",
    "    parser = HfArgumentParser((ModelArguments, DataTrainingArguments, TrainingArguments))\n",
    "    model_args, data_args, training_args = parser.parse_json_file(json_file=json_file_to_parse)\n",
    "\n",
    "\n",
    "    # Detecting last checkpoint.\n",
    "    last_checkpoint = None\n",
    "    if os.path.isdir(training_args.output_dir) and training_args.do_train and not training_args.overwrite_output_dir:\n",
    "        last_checkpoint = get_last_checkpoint(training_args.output_dir)\n",
    "        if last_checkpoint is None and len(os.listdir(training_args.output_dir)) > 0:\n",
    "            raise ValueError(\n",
    "                f\"Output directory ({training_args.output_dir}) already exists and is not empty. \"\n",
    "                \"Use --overwrite_output_dir to overcome.\"\n",
    "            )\n",
    "        elif last_checkpoint is not None and training_args.resume_from_checkpoint is None:\n",
    "            logger.info(\n",
    "                f\"Checkpoint detected, resuming training at {last_checkpoint}. To avoid this behavior, change \"\n",
    "                \"the `--output_dir` or add `--overwrite_output_dir` to train from scratch.\"\n",
    "            )\n",
    "\n",
    "    # Setup logging\n",
    "    logging.basicConfig(\n",
    "        format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\",\n",
    "        datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "        handlers=[logging.StreamHandler(sys.stdout)],\n",
    "    )\n",
    "    logger.setLevel(logging.INFO if training_args.should_log else logging.WARN)\n",
    "\n",
    "    # Log on each process the small summary:\n",
    "    logger.warning(\n",
    "        f\"Process rank: {training_args.local_rank}, device: {training_args.device}, n_gpu: {training_args.n_gpu}\"\n",
    "        + f\"distributed training: {bool(training_args.local_rank != -1)}, 16-bits training: {training_args.fp16}\"\n",
    "    )\n",
    "    # Set the verbosity to info of the Transformers logger (on main process only):\n",
    "    if training_args.should_log:\n",
    "        transformers.utils.logging.set_verbosity_info()\n",
    "        transformers.utils.logging.enable_default_handler()\n",
    "        transformers.utils.logging.enable_explicit_format()\n",
    "    logger.info(f\"Training/evaluation parameters {training_args}\")\n",
    "\n",
    "    \n",
    "    \n",
    "    # data source\n",
    "    data_args.dataset_name_source = args_al.dataset_name\n",
    "    data_source = os.path.join(data_args.main_data_dir,data_args.dataset_name_source)\n",
    "\n",
    "    # set data paths for train, dev and test set\n",
    "    data_args.train_file = os.path.join(data_source,'train.csv')\n",
    "    data_args.validation_file = os.path.join(data_source,'dev.csv')\n",
    "    data_args.test_file = os.path.join(data_source,'test.csv')\n",
    "\n",
    "    # load data\n",
    "    if data_args.dataset_name is not None:\n",
    "\n",
    "        print('s')\n",
    "        datasets = load_dataset(data_args.dataset_name, data_args.dataset_config_name, cache_dir=model_args.cache_dir)\n",
    "    else:\n",
    "\n",
    "        data_files = {}\n",
    "        if data_args.train_file is not None:\n",
    "            data_files[\"train\"] = data_args.train_file\n",
    "            extension = data_args.train_file.split(\".\")[-1]\n",
    "\n",
    "        if data_args.validation_file is not None:\n",
    "            data_files[\"validation\"] = data_args.validation_file\n",
    "            extension = data_args.validation_file.split(\".\")[-1]\n",
    "        if data_args.test_file is not None:\n",
    "            data_files[\"test\"] = data_args.test_file\n",
    "            extension = data_args.test_file.split(\".\")[-1]\n",
    "\n",
    "    # load datasets\n",
    "    raw_datasets = load_dataset(extension, data_files=data_files)\n",
    "\n",
    "    # drop unwanted columns\n",
    "    raw_datasets = raw_datasets.remove_columns(['Unnamed: 0'])\n",
    "\n",
    "\n",
    "    # change column names\n",
    "    raw_datasets = raw_datasets.rename_column('label','labels')\n",
    "\n",
    "    # Labels\n",
    "    if data_args.task_name is not None:\n",
    "        is_regression = data_args.task_name == \"stsb\"\n",
    "        if not is_regression:\n",
    "            label_list = raw_datasets[\"train\"].features[\"label\"].names\n",
    "            num_labels = len(label_list)\n",
    "        else:\n",
    "            num_labels = 1\n",
    "    else:\n",
    "        # Trying to have good defaults here, don't hesitate to tweak to your needs.\n",
    "        is_regression = raw_datasets[\"train\"].features[\"labels\"].dtype in [\"float32\", \"float64\"]\n",
    "        if is_regression:\n",
    "            num_labels = 1\n",
    "        else:\n",
    "            # A useful fast method:\n",
    "            # https://huggingface.co/docs/datasets/package_reference/main_classes.html#datasets.Dataset.unique\n",
    "            label_list = raw_datasets[\"train\"].unique(\"labels\")\n",
    "            label_list.sort()  # Let's sort it for determinism\n",
    "            num_labels = len(label_list)\n",
    "\n",
    "    # tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        model_args.tokenizer_name if model_args.tokenizer_name else model_args.model_name_or_path,\n",
    "        cache_dir=model_args.cache_dir,\n",
    "        use_fast=model_args.use_fast_tokenizer,\n",
    "        revision=model_args.model_revision,\n",
    "        use_auth_token=True if model_args.use_auth_token else None,\n",
    "    )\n",
    "\n",
    "\n",
    "    # configurations for BERT\n",
    "    config = BertConfig.from_pretrained(    \n",
    "        pretrained_model_name_or_path=model_args.model_name_or_path\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "    if data_args.use_adversarial:\n",
    "        # BERT model\n",
    "        model = BertForSequenceClassification_c.from_pretrained(\n",
    "            pretrained_model_name_or_path=model_args.model_name_or_path,\n",
    "            from_tf=False,\n",
    "            config=config,\n",
    "            cache_dir=None\n",
    "        )\n",
    "\n",
    "    else:\n",
    "        # BERT model\n",
    "        model = BertForSequenceClassification.from_pretrained(\n",
    "            pretrained_model_name_or_path=model_args.model_name_or_path,\n",
    "            from_tf=False,\n",
    "            config=config,\n",
    "            cache_dir=None\n",
    "        )\n",
    "\n",
    "        raise NotImplementedError\n",
    "\n",
    "    non_label_column_names = [name for name in raw_datasets[\"train\"].column_names if name != \"labels\"]\n",
    "    custom_choice = True\n",
    "\n",
    "\n",
    "    # Preprocessing the raw_datasets\n",
    "    if data_args.task_name is not None:\n",
    "        sentence1_key, sentence2_key = task_to_keys[data_args.task_name]\n",
    "    else:\n",
    "        # Again, we try to have some nice defaults but don't hesitate to tweak to your use case.\n",
    "        non_label_column_names = [name for name in raw_datasets[\"train\"].column_names if name != \"labels\"]\n",
    "        if \"sentence1\" in non_label_column_names and \"sentence2\" in non_label_column_names:\n",
    "            sentence1_key, sentence2_key = \"sentence1\", \"sentence2\"\n",
    "        else:\n",
    "            if len(non_label_column_names) >= 2:\n",
    "\n",
    "                if custom_choice:\n",
    "                    sentence1_key, sentence2_key = non_label_column_names[1], None\n",
    "                else:\n",
    "                    sentence1_key, sentence2_key = non_label_column_names[:2]\n",
    "            else:\n",
    "                sentence1_key, sentence2_key = non_label_column_names[0], None\n",
    "\n",
    "\n",
    "    # Padding strategy\n",
    "    if data_args.pad_to_max_length:\n",
    "        padding = \"max_length\"\n",
    "    else:\n",
    "        # We will pad later, dynamically at batch creation, to the max sequence length in each batch\n",
    "        padding = False\n",
    "\n",
    "    # Some models have set the order of the labels to use, so let's make sure we do use it.\n",
    "    label_to_id = None\n",
    "    if (\n",
    "        model.config.label2id != PretrainedConfig(num_labels=num_labels).label2id\n",
    "        and data_args.task_name is not None\n",
    "        and not is_regression\n",
    "    ):\n",
    "        # Some have all caps in their config, some don't.\n",
    "        label_name_to_id = {k.lower(): v for k, v in model.config.label2id.items()}\n",
    "        if list(sorted(label_name_to_id.keys())) == list(sorted(label_list)):\n",
    "            label_to_id = {i: int(label_name_to_id[label_list[i]]) for i in range(num_labels)}\n",
    "        else:\n",
    "            logger.warning(\n",
    "                \"Your model seems to have been trained with labels, but they don't match the dataset: \",\n",
    "                f\"model labels: {list(sorted(label_name_to_id.keys()))}, dataset labels: {list(sorted(label_list))}.\"\n",
    "                \"\\nIgnoring the model labels as a result.\",\n",
    "            )\n",
    "\n",
    "    elif data_args.task_name is None and not is_regression:\n",
    "\n",
    "        label_to_id = {v: i for i, v in enumerate(label_list)}\n",
    "\n",
    "\n",
    "    if label_to_id is not None:\n",
    "        model.config.label2id = label_to_id\n",
    "        model.config.id2label = {id: label for label, id in config.label2id.items()}\n",
    "    elif data_args.task_name is not None and not is_regression:\n",
    "        model.config.label2id = {l: i for i, l in enumerate(label_list)}\n",
    "        model.config.id2label = {id: label for label, id in config.label2id.items()}\n",
    "\n",
    "\n",
    "    if data_args.use_adversarial:\n",
    "\n",
    "        clasifier_adversarial = Bert_classifier_adversarial(\n",
    "            bert_seq_class=model\n",
    "\n",
    "        )\n",
    "\n",
    "        # swag adversarial\n",
    "        SWAG_adversarial_m = SWAG_adversarial(\n",
    "            BertForSequenceClassification_c,\n",
    "            no_cov_mat=not True,\n",
    "            max_num_models=20,\n",
    "            num_classes=num_labels,\n",
    "            config=config,\n",
    "            model_args=model_args\n",
    "\n",
    "        )\n",
    "\n",
    "    else:\n",
    "\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "    if data_args.max_seq_length > tokenizer.model_max_length:\n",
    "        logger.warning(\n",
    "            f\"The max_seq_length passed ({data_args.max_seq_length}) is larger than the maximum length for the\"\n",
    "            f\"model ({tokenizer.model_max_length}). Using max_seq_length={tokenizer.model_max_length}.\"\n",
    "        )\n",
    "    max_seq_length = min(data_args.max_seq_length, tokenizer.model_max_length)\n",
    "    \n",
    "    raw_datasets_init = raw_datasets.copy()\n",
    "\n",
    "    raw_datasets = raw_datasets.map(\n",
    "        preprocess_function,\n",
    "        batched=True,\n",
    "        load_from_cache_file=not data_args.overwrite_cache,\n",
    "    #     desc=\"Running tokenizer on dataset\",\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "    # datasets and reducing size if needed\n",
    "    if training_args.do_train:\n",
    "        if \"train\" not in raw_datasets:\n",
    "            raise ValueError(\"--do_train requires a train dataset\")\n",
    "        train_dataset = raw_datasets[\"train\"]\n",
    "        if data_args.max_train_samples is not None:\n",
    "            train_dataset = train_dataset.select(range(data_args.max_train_samples))\n",
    "\n",
    "    if training_args.do_eval:\n",
    "        if \"validation\" not in raw_datasets and \"validation_matched\" not in raw_datasets:\n",
    "            raise ValueError(\"--do_eval requires a validation dataset\")\n",
    "        eval_dataset = raw_datasets[\"validation_matched\" if data_args.task_name == \"mnli\" else \"validation\"]\n",
    "        if data_args.max_eval_samples is not None:\n",
    "            eval_dataset = eval_dataset.select(range(data_args.max_eval_samples))\n",
    "\n",
    "    if training_args.do_predict or data_args.task_name is not None or data_args.test_file is not None:\n",
    "        if \"test\" not in raw_datasets and \"test_matched\" not in raw_datasets:\n",
    "            raise ValueError(\"--do_predict requires a test dataset\")\n",
    "        predict_dataset = raw_datasets[\"test_matched\" if data_args.task_name == \"mnli\" else \"test\"]\n",
    "        if data_args.max_predict_samples is not None:\n",
    "            predict_dataset = predict_dataset.select(range(data_args.max_predict_samples))\n",
    "\n",
    "\n",
    "    # Log a few random samples from the training set:\n",
    "    if training_args.do_train:\n",
    "        for index in random.sample(range(len(train_dataset)), 3):\n",
    "            logger.info(f\"Sample {index} of the training set: {train_dataset[index]}.\")\n",
    "\n",
    "    # Get the metric function\n",
    "    if data_args.task_name is not None:\n",
    "        metric = load_metric(\"glue\", data_args.task_name)\n",
    "    else:\n",
    "        metric = load_metric(\"accuracy\")\n",
    "\n",
    "    # Data collator will default to DataCollatorWithPadding, so we change it if we already did the padding.\n",
    "    if data_args.pad_to_max_length:\n",
    "        data_collator = default_data_collator\n",
    "    elif training_args.fp16:\n",
    "        data_collator = DataCollatorWithPadding(tokenizer, pad_to_multiple_of=8)\n",
    "    else:\n",
    "        data_collator = None\n",
    "        \n",
    "        \n",
    "    # criterion for loss function\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "\n",
    "    # use only one aquisition\n",
    "    ALL_AQUISITIONS = [args_al.aquisition_trategy]\n",
    "    \n",
    "#     # modify\n",
    "#     average_results = generate_active_lr_results(\n",
    "#         trials_strategy=args_al.trials_strategy,\n",
    "#         n_epochs_train=args_al.n_epochs_train,\n",
    "#         T_aq_uncert=args_al.T_aq_uncert,\n",
    "#         n_query_pick=args_al.n_query_pick,\n",
    "#         bays_adv_approach=args_al.bays_adv_approach,\n",
    "#         active_learning_iters=args_al.active_learning_iters,\n",
    "#         initial_train_data_size=args_al.initial_train_data_size,\n",
    "#         n_sample_size_pool_inference=args_al.percentage_sample_size_pool_inference\n",
    "#         use_swag=args_al.use_swag,\n",
    "#         use_clustering=args_al.use_clustering,\n",
    "#         initial_train_seed=args_al.initial_train_seed,\n",
    "#         cluster_seed=args_al.cluster_seed,\n",
    "#         mc_dropout=args_al.mc_dropout,\n",
    "#         train_mode=args_al.train_mode,\n",
    "#         cl_m=args_al.clustering_strategy,\n",
    "#         name_exp=args_al.experiment_name,\n",
    "#         low_rank=args_al.low_rank,\n",
    "#         dataset_name=args_al.dataset_name\n",
    "    \n",
    "#     )\n",
    "    \n",
    "    \n",
    "#     path_save = args_al.path_save\n",
    "    \n",
    "\n",
    "#     if not os.path.exists(path_save):\n",
    "#         os.makedirs(path_save)\n",
    "\n",
    "#     # save normal results\n",
    "#     name = args_al.experiment_name + '.json'\n",
    "    \n",
    "#     with open(os.path.join(path_save,name), 'w') as f:\n",
    "#         json.dump(average_results, f)\n",
    "    \n",
    "    # example run file with bays aproach\n",
    "    #python3 run_compare_clust.py -ts=20 -e=20 -taq=50 -nq=10 -al_it=10 -init_ts=20 -pss=400 -aq_s=bald -path_s=final_experiments -exp_n=bays_culst_20_trials -badv -swag -clus\n",
    "    \n",
    "    # example run file without bays aproach\n",
    "    #python3 run_compare_clust.py -ts=20 -e=20 -taq=50 -nq=10 -al_it=10 -init_ts=20 -pss=400 -aq_s=bald -path_s=final_experiments -exp_n=bays_culst_20_trials -clus\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    #ALL_AQUISITIONS = ALL_AQUISITIONS[:2]\n",
    "    \n",
    "#     parser = argparse.ArgumentParser()\n",
    "    \n",
    "#     parser.add_argument(\n",
    "#         \"--useclustering\",\n",
    "#         default=False,\n",
    "#         action='store_true',\n",
    "#         help=\"whether to use clustering approach\",\n",
    "#     )\n",
    "    \n",
    "#     parser.add_argument(\n",
    "#         \"--epochs\",\n",
    "#         type=int,\n",
    "#         default=10,\n",
    "#         metavar=\"EP\",\n",
    "#         help=\"number of epochs to train (default: 50)\",\n",
    "#     )\n",
    "    \n",
    "#     parser.add_argument(\n",
    "#         \"--aliters\",\n",
    "#         type=int,\n",
    "#         default=10,\n",
    "#         metavar=\"alI\",\n",
    "#         help=\"number of active learning iterations\",\n",
    "#     )\n",
    "    \n",
    "#     parser.add_argument(\n",
    "#         \"--lr\",\n",
    "#         type=float,\n",
    "#         default=1e-3,\n",
    "#         metavar=\"LR\",\n",
    "#         help=\"learning rate (default: 1e-3)\",\n",
    "#     )\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "# #     # normal model\n",
    "#     average_results_n = generate_active_lr_results(\n",
    "#         trials_strategy=params_active_l_n[\"trials_strategy\"],\n",
    "#         n_epochs_train=params_active_l_n[\"n_epochs_train\"],\n",
    "#         T_aq_uncert=params_active_l_n[\"T_aq_uncert\"],\n",
    "#         n_query_pick=params_active_l_n[\"n_query_pick\"],\n",
    "#         bays_adv_approach=params_active_l_n[\"bays_adv_approach\"],\n",
    "#         active_learning_iters=params_active_l_n[\"active_learning_iters\"],\n",
    "#         initial_train_data_size=params_active_l_n[\"initial_train_data_size\"],\n",
    "#         n_sample_size_pool_inference=params_active_l_n[\"n_sample_size_pool_inference\"],\n",
    "#         use_swag=params_active_l_n[\"use_swag\"]\n",
    "        \n",
    "#     )\n",
    "    \n",
    "    \n",
    "#     # bay_adversarial model\n",
    "#     average_results_b_adv = generate_active_lr_results(\n",
    "#         trials_strategy=params_active_l_b[\"trials_strategy\"],\n",
    "#         n_epochs_train=params_active_l_b[\"n_epochs_train\"],\n",
    "#         T_aq_uncert=params_active_l_b[\"T_aq_uncert\"],\n",
    "#         n_query_pick=params_active_l_b[\"n_query_pick\"],\n",
    "#         bays_adv_approach=params_active_l_b[\"bays_adv_approach\"],\n",
    "#         active_learning_iters=params_active_l_b[\"active_learning_iters\"],\n",
    "#         initial_train_data_size=params_active_l_b[\"initial_train_data_size\"],\n",
    "#         n_sample_size_pool_inference=params_active_l_b[\"n_sample_size_pool_inference\"],\n",
    "#         use_swag=params_active_l_b[\"use_swag\"]\n",
    "#     )\n",
    "    \n",
    "    \n",
    "    \n",
    "#     path_save = './results_al/'\n",
    "\n",
    "#     if not os.path.exists(path_save):\n",
    "#         os.makedirs(path_save)\n",
    "\n",
    "#     # save normal results\n",
    "#     name = params_active_l_n['name']\n",
    "#     with open(os.path.join(path_save,name), 'w') as f:\n",
    "#         json.dump(average_results_n, f)\n",
    "        \n",
    "#     # save bays_adv results\n",
    "#     name_b = params_active_l_b['name']\n",
    "#     with open(os.path.join(path_save,name_b), 'w') as f:\n",
    "#         json.dump(average_results_b_adv, f)\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min(data_args.max_seq_length, tokenizer.model_max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_args.overwrite_cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_args.task_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_args.task_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_labels = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training_args.should_save = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args._n_gpu = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# args.should_save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cl_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_datasets['train'].to_pandas().labels.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls available_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args_al.low_rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "average_results = generate_active_lr_results(\n",
    "    trials_strategy=args_al.trials_strategy,\n",
    "    n_epochs_train=args_al.n_epochs_train,\n",
    "    T_aq_uncert=2,\n",
    "    n_query_pick=args_al.n_query_pick,\n",
    "    bays_adv_approach=args_al.bays_adv_approach,\n",
    "    active_learning_iters=args_al.active_learning_iters,\n",
    "    initial_train_data_size=args_al.initial_train_data_size,\n",
    "    n_sample_size_pool_inference=args_al.percentage_sample_size_pool_inference,\n",
    "    use_swag=args_al.use_swag,\n",
    "    use_clustering=args_al.use_clustering,\n",
    "    initial_train_seed=args_al.initial_train_seed,\n",
    "    cluster_seed=args_al.cluster_seed,\n",
    "    mc_dropout=args_al.mc_dropout,\n",
    "    train_mode=args_al.train_mode,\n",
    "    cl_m=args_al.clustering_strategy,\n",
    "    low_rank=args_al.low_rank,\n",
    "    dataset_name=args_al.dataset_name\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test/debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Namespace(T_aq_uncert=50, active_learning_iters=13, aquisition_trategy='bald', bays_adv_approach=False, cluster_seed=16, clustering_strategy='high', dataset_name='cola', experiment_name='temp_ts_10_e_20_taq_50_nq_40_init_ts_100_pss_300_its_33_norm_bald_mc_60_clust', initial_train_data_size=20, initial_train_seed=30, low_rank=False, mc_dropout=True, n_epochs_train=20, n_query_pick=10, path_save=PosixPath('final_paper_experiments'), percentage_sample_size_pool_inference=0.05, train_mode=True, trials_strategy=10, use_clustering=True, use_swag=False)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args_al"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "trials_strategy=args_al.trials_strategy\n",
    "n_epochs_train=args_al.n_epochs_train\n",
    "T_aq_uncert=args_al.T_aq_uncert\n",
    "n_query_pick=args_al.n_query_pick\n",
    "bays_adv_approach=args_al.bays_adv_approach\n",
    "active_learning_iters=args_al.active_learning_iters\n",
    "initial_train_data_size=args_al.initial_train_data_size\n",
    "n_sample_size_pool_inference=args_al.percentage_sample_size_pool_inference\n",
    "use_swag=args_al.use_swag\n",
    "use_clustering=args_al.use_clustering\n",
    "initial_train_seed=args_al.initial_train_seed\n",
    "cluster_seed=args_al.cluster_seed\n",
    "mc_dropout=args_al.mc_dropout\n",
    "train_mode=args_al.train_mode\n",
    "dataset_name=args_al.train_mode\n",
    "low_rank=args_al.low_rank\n",
    "dataset_name=args_al.dataset_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_epochs_train = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, True)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mc_dropout,use_clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use_clustering= False\n",
    "# n_epochs_train=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bald']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ALL_AQUISITIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|configuration_utils.py:581] 2022-01-03 12:29:22,567 >> loading configuration file /gpfs/cfms/home/cconstantinou/.cache/torch/sentence_transformers/sentence-transformers_multi-qa-MiniLM-L6-cos-v1/config.json\n",
      "[INFO|configuration_utils.py:620] 2022-01-03 12:29:22,570 >> Model config BertConfig {\n",
      "  \"_name_or_path\": \"nreimers/MiniLM-L6-H384-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 384,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 1536,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.11.0.dev0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "[INFO|modeling_utils.py:1321] 2022-01-03 12:29:23,606 >> loading weights file /gpfs/cfms/home/cconstantinou/.cache/torch/sentence_transformers/sentence-transformers_multi-qa-MiniLM-L6-cos-v1/pytorch_model.bin\n",
      "[INFO|modeling_utils.py:1588] 2022-01-03 12:29:24,370 >> All model checkpoint weights were used when initializing BertModel.\n",
      "\n",
      "[INFO|modeling_utils.py:1597] 2022-01-03 12:29:24,371 >> All the weights of BertModel were initialized from the model checkpoint at /gpfs/cfms/home/cconstantinou/.cache/torch/sentence_transformers/sentence-transformers_multi-qa-MiniLM-L6-cos-v1/.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.\n",
      "[INFO|tokenization_utils_base.py:1671] 2022-01-03 12:29:24,376 >> Didn't find file /gpfs/cfms/home/cconstantinou/.cache/torch/sentence_transformers/sentence-transformers_multi-qa-MiniLM-L6-cos-v1/added_tokens.json. We won't load it.\n",
      "[INFO|tokenization_utils_base.py:1739] 2022-01-03 12:29:24,377 >> loading file /gpfs/cfms/home/cconstantinou/.cache/torch/sentence_transformers/sentence-transformers_multi-qa-MiniLM-L6-cos-v1/vocab.txt\n",
      "[INFO|tokenization_utils_base.py:1739] 2022-01-03 12:29:24,378 >> loading file /gpfs/cfms/home/cconstantinou/.cache/torch/sentence_transformers/sentence-transformers_multi-qa-MiniLM-L6-cos-v1/tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:1739] 2022-01-03 12:29:24,378 >> loading file None\n",
      "[INFO|tokenization_utils_base.py:1739] 2022-01-03 12:29:24,379 >> loading file /gpfs/cfms/home/cconstantinou/.cache/torch/sentence_transformers/sentence-transformers_multi-qa-MiniLM-L6-cos-v1/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:1739] 2022-01-03 12:29:24,380 >> loading file /gpfs/cfms/home/cconstantinou/.cache/torch/sentence_transformers/sentence-transformers_multi-qa-MiniLM-L6-cos-v1/tokenizer_config.json\n",
      "/gpfs/cfms/home/cconstantinou/miniconda3/envs/packclone/lib/python3.6/site-packages/transformers/tokenization_utils_base.py:2227: UserWarning: `max_length` is ignored when `padding`=`True`.\n",
      "  warnings.warn(\"`max_length` is ignored when `padding`=`True`.\")\n"
     ]
    }
   ],
   "source": [
    "average_results = {i:{} for i in ALL_AQUISITIONS}\n",
    "\n",
    "if use_clustering:\n",
    "    indexs_per_cluster = get_clustered_indexes_dict(raw_datasets_init,rand_state=cluster_seed)\n",
    "else:\n",
    "    indexs_per_cluster = None\n",
    "\n",
    "num_experiemnts = len(ALL_AQUISITIONS) * trials_strategy \n",
    "n_done = 0\n",
    "\n",
    "\n",
    "all_trials = {}\n",
    "\n",
    "assl_trinals_randoms = {}\n",
    "\n",
    "for strategey in ALL_AQUISITIONS:\n",
    "\n",
    "    # matrices to average results\n",
    "    all_trials_ac_performace = np.zeros((trials_strategy,active_learning_iters)) \n",
    "    all_trials_b_ac_performace = np.zeros((trials_strategy,active_learning_iters)) \n",
    "    all_trials_f1_performace = np.zeros((trials_strategy,active_learning_iters)) \n",
    "\n",
    "\n",
    "    for trial in range(trials_strategy):\n",
    "        \n",
    "        \n",
    "        break\n",
    "\n",
    "  \n",
    "#     all_trials['acc'] = all_trials_ac_performace\n",
    "#     all_trials['b_acc'] = all_trials_b_ac_performace\n",
    "#     all_trials['f1'] = all_trials_f1_performace\n",
    "#     # average results\n",
    "#     average_results[strategey]['acc'] = all_trials_ac_performace.mean(axis=0).tolist()\n",
    "#     average_results[strategey]['b_acc'] = all_trials_b_ac_performace.mean(axis=0).tolist()\n",
    "#     average_results[strategey]['f1'] = all_trials_f1_performace.mean(axis=0).tolist()\n",
    "#     average_results[strategey]['n_train'] = number_of_train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Jan  3 12:29:56 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 460.91.03    Driver Version: 460.91.03    CUDA Version: 11.2     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla M60           Off  | 00000000:00:0B.0 Off |                  Off |\n",
      "| N/A   36C    P0    38W / 150W |   2186MiB /  8129MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  Tesla M60           Off  | 00000000:00:0C.0 Off |                  Off |\n",
      "| N/A   33C    P0    38W / 150W |    782MiB /  8129MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A     12553      C   ...3/envs/testenv/bin/python      719MiB |\n",
      "|    0   N/A  N/A     16502      C   ...3/envs/testenv/bin/python      739MiB |\n",
      "|    0   N/A  N/A     21886      C   ...3/envs/testenv/bin/python      721MiB |\n",
      "|    1   N/A  N/A     13768      C   ...envs/packclone/bin/python      779MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({}, 0)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "assl_trinals_randoms,trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assl_trinals_randoms[trial] = []\n",
    "\n",
    "# data handler\n",
    "data_handler = Data_Handler(\n",
    "    train_dataset,\n",
    "    eval_dataset,\n",
    "    predict_dataset,\n",
    "    indexs_per_cluster\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args.num_train_epochs = n_epochs_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "active_model = Active_learner(\n",
    "    data_handler=data_handler,\n",
    "    data_args=data_args,\n",
    "    model_args=model_args,\n",
    "    training_args=training_args,\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    criterion=criterion,\n",
    "    aquisiiton_f=aquisition_function,\n",
    "    num_labels=num_labels,\n",
    "    bays_aversarial=bays_adv_approach,\n",
    "    iters=active_learning_iters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dir(active_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_train_data_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_train_data_size = 100\n",
    "# initial_train_data_size = 100000000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_n_train=initial_train_data_size\n",
    "n_sample_infer=n_sample_size_pool_inference\n",
    "T_aq=T_aq_uncert\n",
    "n_query=n_query_pick\n",
    "aquisition_strategy=strategey\n",
    "use_swag=use_swag\n",
    "r_s=trial\n",
    "initial_train_seed=initial_train_seed\n",
    "mc_dropout=mc_dropout\n",
    "train_mode=train_mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_train_seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_performance_ac = []\n",
    "test_performance_f1 = []\n",
    "test_performance_b_ac = []\n",
    "\n",
    "\n",
    "number_of_train_data = []\n",
    "\n",
    "# original test data\n",
    "test_data_for_inference = active_model.data_handler.original_test\n",
    "\n",
    "\n",
    "# initialize train data and pool data\n",
    "active_model.data_handler.initialize_train_pool(n_init_train=initial_n_train,r_seed=initial_train_seed)\n",
    "\n",
    "\n",
    "print('data info:')\n",
    "print(f'pool data size: {len(active_model.data_handler.indexes_pool)}')\n",
    "print(f'train data size: {len(active_model.data_handler.indexes_train)}')\n",
    "\n",
    "\n",
    "for iterr in range(active_model.n_iters):\n",
    "    \n",
    "    break\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get train data for training\n",
    "c_train_data = active_model.data_handler.get_train_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_train_data.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "active_model.initialize_model_trainer(c_train_data=c_train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "number_of_points_used_in_training = c_train_data.shape[0]\n",
    "number_of_points_used_in_training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "active_model.my_trainer.model.bert.__dict__['_modules'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# active_model.my_trainer.args.should_save = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#train trainer with chosen data\n",
    "active_model.my_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# active_model.my_trainer.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# active_model.my_trainer.swag_model.n_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dir(active_model.my_trainer.swag_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 's'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# active_model.my_trainer.swag_model.sample(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls kag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# active_model.my_trainer.model.load_state_dict(torch.load('./kag/torch_bin.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# active_model.my_trainer.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_swag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ac,f1,b_acc,all_logits = active_model.evaluate_model(\n",
    "    dataset=test_data_for_inference,\n",
    "    swag=use_swag\n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### explore start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "active_model.my_trainer.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "active_model.bays_aversarial,use_swag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "active_model.my_trainer.use_adversarial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score,balanced_accuracy_score,accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### fere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = active_model.my_trainer.swag_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.sample(0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.__dict__['_modules']['base'].bert.bert.embeddings.word_embeddings.__dict__['weight']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.__dict__['_modules']['base'].bert.bert.embeddings.word_embeddings.__dict__['_buffers']['weight_mean']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.sample(100.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.__dict__['_modules']['base'].bert.bert.embeddings.word_embeddings.__dict__['weight']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model.__dict__['_modules']['base'].bert.bert.embeddings.word_embeddings.__dict__['_buffers']['weight_mean']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.__dict__['_modules']['base'].bert.bert.embeddings.word_embeddings.__dict__['_buffers']['weight_mean'][0][0].item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.__dict__['_modules']['base'].bert.bert.embeddings.word_embeddings.__dict__['weight'][0][0].item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.__dict__['_modules']['base'].bert.bert.embeddings.word_embeddings.__dict__['weight'].to('cpu') == model.__dict__['_modules']['base'].bert.bert.embeddings.word_embeddings.__dict__['_buffers']['weight_mean']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(model.__dict__['_modules']['base'].bert.bert.embeddings.word_embeddings.__dict__['weight'].to('cpu') == model.__dict__['_modules']['base'].bert.bert.embeddings.word_embeddings.__dict__['_buffers']['weight_mean']).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = active_model.my_trainer.swag_model\n",
    "model.sample(10000.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = active_model.my_trainer.model\n",
    "model.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "test_dataloader = active_model.my_trainer.get_test_dataloader(test_data_for_inference)    \n",
    "# lists to store predictions and groud trtuh values\n",
    "all_labels_true = []\n",
    "all_preds = []\n",
    "all_logits = []\n",
    "\n",
    "ignore_keys = None\n",
    "prediction_loss_only = None\n",
    "\n",
    "# progress bar\n",
    "prediction_bar = tqdm(total=len(test_dataloader))\n",
    "\n",
    "\n",
    "model.train()\n",
    "\n",
    "for step, inputs in enumerate(test_dataloader):\n",
    "\n",
    "\n",
    "    loss, logits, labels = active_model.my_trainer.prediction_step_bidir_adversarial(model, \n",
    "                                                            inputs, \n",
    "                                                            prediction_loss_only, \n",
    "                                                            ignore_keys=ignore_keys)\n",
    "\n",
    "    all_preds.extend(logits.argmax(axis=1).cpu().numpy().tolist())\n",
    "    all_labels_true.extend(labels.cpu().numpy().tolist())\n",
    "    all_logits.append(logits.cpu())\n",
    "\n",
    "    prediction_bar.update(1)\n",
    "\n",
    "\n",
    "array_true = np.array(all_labels_true)\n",
    "array_pred = np.array(all_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "array_true.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1 = f1_score(array_true,array_pred,average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1 = f1_score(array_true,array_pred,average='weighted')\n",
    "f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1 = f1_score(array_true,array_pred,average='weighted')\n",
    "f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1 = f1_score(array_true,array_pred,average='weighted')\n",
    "f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.uniform(0,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir(active_model.my_trainer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "active_model.my_trainer.n_collect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataloader = self.my_trainer.get_test_dataloader(dataset)    \n",
    "\n",
    "# lists to store predictions and groud trtuh values\n",
    "all_labels_true = []\n",
    "all_preds = []\n",
    "all_logits = []\n",
    "\n",
    "\n",
    "# progress bar\n",
    "prediction_bar = tqdm(total=len(test_dataloader))\n",
    "\n",
    "model.eval()\n",
    "\n",
    "for step, inputs in enumerate(test_dataloader):\n",
    "\n",
    "\n",
    "    loss, logits, labels = self.my_trainer.prediction_step_bidir_adversarial(model, \n",
    "                                                            inputs, \n",
    "                                                            prediction_loss_only, \n",
    "                                                            ignore_keys=ignore_keys)\n",
    "\n",
    "    all_preds.extend(logits.argmax(axis=1).cpu().numpy().tolist())\n",
    "    all_labels_true.extend(labels.cpu().numpy().tolist())\n",
    "    all_logits.append(logits.cpu())\n",
    "\n",
    "    prediction_bar.update(1)\n",
    "\n",
    "\n",
    "array_true = np.array(all_labels_true)\n",
    "array_pred = np.array(all_preds)\n",
    "\n",
    "# accuracy score\n",
    "ac = accuracy_score(array_true,array_pred)\n",
    "# weighted f1 score\n",
    "f1 = f1_score(array_true,array_pred,average='weighted')\n",
    "#balanced accuracy score\n",
    "b_acc = balanced_accuracy_score(array_true,array_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### explore end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.cat(all_logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_features = torch.cat(all_logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (all_features.exp()/ all_features.exp().sum(axis=1).view(-1,1)).sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_features_prob = (all_features.exp()/ all_features.exp().sum(axis=1).view(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_features_prob[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# index_top = all_features_prob.argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# most_probable = [i[j].item() for i,j in zip(all_features_prob,index_top)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_features.exp().sum(axis=1).view(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_features_prob_nump = all_features_prob.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_feats = pd.DataFrame(data=all_features_prob_nump,columns=[f'feat_{i}' for i in range(all_features_prob_nump.shape[1])])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls ../../../../kag/toxicity/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_feats.to_csv('../../../../kag/toxicity/processed_data/feats_xgb_to_score.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls ../../../../kag/toxicity/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset=test_data_for_inference\n",
    "# swag=use_swag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# active_model.my_trainer.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "active_model.my_trainer.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mkdir kag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(active_model.my_trainer.model.state_dict(), './kag/torch_bin.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls kag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score,balanced_accuracy_score,accuracy_score\n",
    "swag=use_swag\n",
    "dataset=test_data_for_inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "swag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ignore_keys = None\n",
    "prediction_loss_only = None\n",
    "\n",
    "if active_model.bays_aversarial:\n",
    "\n",
    "    if swag:\n",
    "        model = active_model.my_trainer.swag_model\n",
    "        model.sample(20.0,True)\n",
    "\n",
    "    else:\n",
    "        model = active_model.my_trainer.model\n",
    "        model.to('cuda')\n",
    "else:\n",
    "    model = active_model.my_trainer.model\n",
    "    model.to('cuda')\n",
    "\n",
    "\n",
    "test_dataloader = active_model.my_trainer.get_test_dataloader(dataset)    \n",
    "\n",
    "# lists to store predictions and groud trtuh values\n",
    "all_labels_true = []\n",
    "all_preds = []\n",
    "all_logits = []\n",
    "\n",
    "\n",
    "# progress bar\n",
    "prediction_bar = tqdm(total=len(test_dataloader))\n",
    "\n",
    "model.eval()\n",
    "\n",
    "for step, inputs in enumerate(test_dataloader):\n",
    "\n",
    "\n",
    "    loss, logits, labels = active_model.my_trainer.prediction_step_bidir_adversarial(model, \n",
    "                                                            inputs, \n",
    "                                                            prediction_loss_only, \n",
    "                                                            ignore_keys=ignore_keys)\n",
    "\n",
    "    all_preds.extend(logits.argmax(axis=1).cpu().numpy().tolist())\n",
    "    all_labels_true.extend(labels.cpu().numpy().tolist())\n",
    "    all_logits.append(logits.cpu())\n",
    "\n",
    "    prediction_bar.update(1)\n",
    "\n",
    "\n",
    "array_true = np.array(all_labels_true)\n",
    "array_pred = np.array(all_preds)\n",
    "\n",
    "# accuracy score\n",
    "ac = accuracy_score(array_true,array_pred)\n",
    "# weighted f1 score\n",
    "f1 = f1_score(array_true,array_pred,average='weighted')\n",
    "#balanced accuracy score\n",
    "b_acc = balanced_accuracy_score(array_true,array_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "f1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_sample_size_pool_inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "n_sample_infer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sizes = {\"cola\":7592,\n",
    "              \"polarity\":7463,\n",
    "              \"subjectivity\":7000,\n",
    "              \"ag_news\":15000,\n",
    "              \"pubmed\":49820\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_sample_size_pool_inference = int(data_sizes[dataset_name] * n_sample_size_pool_inference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_sample_size_pool_inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we sample from train data pool for further labeling\n",
    "sample_for_inference = active_model.data_handler.sample_pool(n_sample=n_sample_size_pool_inference)\n",
    "sample_for_inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mc_dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T_aq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_for_inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_preds,all_trus,all_logits = active_model.model_predict(\n",
    "    dataset=sample_for_inference,\n",
    "    swag=use_swag,\n",
    "    T=T_aq,\n",
    "    training=train_mode,\n",
    "    mc_dropout=mc_dropout\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_data_for_inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "use_swag,train_mode,mc_dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score,balanced_accuracy_score,accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### open predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T_aq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ignore_keys = None\n",
    "prediction_loss_only = None\n",
    "\n",
    "\n",
    "test_dataloader = active_model.my_trainer.get_test_dataloader(test_data_for_inference)   \n",
    "\n",
    "# lists to store predictions and groud trtuh values\n",
    "all_trus = []\n",
    "all_preds = []\n",
    "all_logits = []\n",
    "\n",
    "\n",
    "# progress bar\n",
    "prediction_bar = tqdm(total=len(test_dataloader) * T_aq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = active_model.my_trainer.model\n",
    "model.to('cuda')\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mc_dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_preds = []\n",
    "temp_logits = []\n",
    "temp_true = []\n",
    "\n",
    "for step, inputs in enumerate(test_dataloader):\n",
    "\n",
    "\n",
    "    loss, logits, labels = active_model.my_trainer.prediction_step_bidir_adversarial(model, \n",
    "                                                            inputs, \n",
    "                                                            prediction_loss_only, \n",
    "                                                            ignore_keys=ignore_keys)\n",
    "\n",
    "\n",
    "\n",
    "    temp_logits.append(logits.cpu())\n",
    "    temp_preds.append(logits.argmax(axis=1).cpu().numpy())\n",
    "    temp_true.append(labels.cpu().numpy())\n",
    "\n",
    "    prediction_bar.update(1)\n",
    "\n",
    "\n",
    "all_preds.append(np.concatenate(temp_preds,axis=0))\n",
    "all_logits.append(np.concatenate(temp_logits,axis=0))\n",
    "all_trus.append(np.concatenate(temp_true,axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(np.concatenate(temp_preds,axis=0) == np.concatenate(temp_true,axis=0)).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score(np.concatenate(temp_true,axis=0),np.concatenate(temp_preds,axis=0),average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(all_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_preds[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_allll = np.vstack(all_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_allll.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(all_trus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_on_t_data_a = [f1_score(all_trus[i],all_preds[i],average='weighted') for i in range(len(all_trus))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scores_on_t_data_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame({'vals':scores_on_t_data}).vals.hist().plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.DataFrame({'vals':scores_on_t_data_a}).vals.hist().plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pd.DataFrame({'vals':scores_on_t_data_a}).vals.hist().plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame({'vals':scores_on_t_data_a}).vals.hist().plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_on_t_data_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_allll[:,3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_allll.std(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_allll.std(axis=0)[chosen_indexes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sorted(preds_allll.std(axis=0),reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "active_model.aquisiiton_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aquisition_strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chosen_indexes = active_model.aquisiiton_f(logits=all_logits,\n",
    "                                   n_query=n_query,\n",
    "                                   type_=aquisition_strategy\n",
    "                                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chosen_indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(chosen_indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir(active_model.data_handler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[len(active_model.data_handler.clustered_idx_pool[i]) for i in active_model.data_handler.clustered_idx_pool],sum([len(active_model.data_handler.clustered_idx_pool[i]) for i in active_model.data_handler.clustered_idx_pool])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update the pool and train data in the data handler\n",
    "active_model.data_handler.update_pool_train_indexes(chosen_indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "active_model.data_handler.indexes_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chosen_indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[len(active_model.data_handler.clustered_idx_pool[i]) for i in active_model.data_handler.clustered_idx_pool],sum([len(active_model.data_handler.clustered_idx_pool[i]) for i in active_model.data_handler.clustered_idx_pool])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_train_data = active_model.data_handler.get_train_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "c_train_data.to_pandas().labels.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[  337   347   939   977  1380  2434  4588  4982  5256  5472  6197  6431\n",
    "  8049  8572 11482 12355 13351 13949 14042 14709]\n",
    "\n",
    "[  337   347   939   977  1380  2434  4588  4982  5256  5472  6197  6431\n",
    "  8049  8572 11482 12355 13351 13949 14042 14709]\n",
    "0%|██████████████████████████████████████████████████████████████████████████████████████| 94/94 [00:20<00:00,  4.53it/s]\n",
    "iterr: 0, f1: 0.6784161897251807\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  AP - Everything from fire ants and coyotes to ...\n",
    "1     When Alonzo Jackson had his much-publicized fi...\n",
    "2     A Yemen court has sentenced 15 men on terror c...\n",
    "3     Oklahoma moved up to No. 2 in the Bowl Champio...\n",
    "4     On-demand viewing isn't just for TiVo owners a...\n",
    "5     India 's fear of a downstream deluge in the Su...\n",
    "6     AFP - Shares in China's largest personal compu...\n",
    "7     Robbie Keane was dropped as dreadful Spurs cra...\n",
    "8     JOSE MOURINHO last night promised Barcelona th...\n",
    "9     PARIS (Reuters) - Europe 's largest carrier Ai...\n",
    "10    BESLAN: Flags are flying at half-mast across R...\n",
    "11    Sydney commuters get a day's free travel after...\n",
    "12    Hurricane Frances may have almost doubled the ...\n",
    "13    Over 100,000 refugees in Zambia will have thei...\n",
    "14    Bee Staff Writer. They don 't just remember th...\n",
    "15    NAJAF, Iraq (AP) Sporadic gunfire echoed throu...\n",
    "16    Nokia on Wednesday said it will put hologram l...\n",
    "17    Reuters - The scandal at Marsh   McLennan Cos....\n",
    "18    NEW YORK (CBS.MW) - Media stocks opened mixed ...\n",
    "        \n",
    "    \n",
    "    iterr: 0, f1: 0.6672990745783942"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_train_data.to_pandas().text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get train data for training\n",
    "c_train_data = active_model.data_handler.get_train_data()\n",
    "\n",
    "# initialize models and trainer\n",
    "active_model.initialize_model_trainer(c_train_data=c_train_data)\n",
    "\n",
    "# number of points used for training\n",
    "number_of_points_used_in_training = c_train_data.shape[0]\n",
    "\n",
    "#train trainer with chosen data\n",
    "active_model.my_trainer.train()\n",
    "\n",
    "\n",
    "#evaluate trained model\n",
    "ac,f1,b_acc,all_logits = active_model.evaluate_model(\n",
    "    dataset=test_data_for_inference,\n",
    "    swag=use_swag\n",
    "          )\n",
    "\n",
    "print(f'iterr: {iterr}, f1: {f1}')\n",
    "\n",
    "# store the performance values and the number of train data used\n",
    "test_performance_ac.append(ac)\n",
    "test_performance_f1.append(f1)\n",
    "test_performance_b_ac.append(b_acc)\n",
    "number_of_train_data.append(number_of_points_used_in_training)\n",
    "\n",
    "np.random.seed(r_s)\n",
    "random.seed(r_s)\n",
    "\n",
    "# we sample from train data pool for further labeling\n",
    "sample_for_inference = active_model.data_handler.sample_pool(n_sample=n_sample_infer)\n",
    "\n",
    "# perform inference on sampled data\n",
    "all_preds,all_trus,all_logits = active_model.model_predict(\n",
    "    dataset=sample_for_inference,\n",
    "    swag=use_swag,\n",
    "    T=T_aq,\n",
    "    training=train_mode,\n",
    "    mc_dropout=mc_dropout\n",
    "         )\n",
    "\n",
    "# use aquisition function on infered sample data from pool\n",
    "chosen_indexes = active_model.aquisiiton_f(logits=all_logits,\n",
    "                                   n_query=n_query,\n",
    "                                   type_=aquisition_strategy\n",
    "                                  )\n",
    "\n",
    "\n",
    "# update the pool and train data in the data handler\n",
    "active_model.data_handler.update_pool_train_indexes(chosen_indexes)\n",
    "\n",
    "\n",
    "print('data info:')\n",
    "print(f'pool data size: {len(active_model.data_handler.indexes_pool)}')\n",
    "print(f'train data size: {len(active_model.data_handler.indexes_train)}')\n",
    "\n",
    "\n",
    "active_model.data_handler.sanity_check()\n",
    "print('SANITY CHECK passed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assl_trinals_randoms[trial] = []\n",
    "\n",
    "# data handler\n",
    "data_handler = Data_Handler(\n",
    "    train_dataset,\n",
    "    eval_dataset,\n",
    "    predict_dataset,\n",
    "    indexs_per_cluster\n",
    ")\n",
    "\n",
    "\n",
    "# epochs used for training\n",
    "training_args.num_train_epochs = n_epochs_train\n",
    "\n",
    "# active learner\n",
    "active_model = Active_learner(\n",
    "    data_handler=data_handler,\n",
    "    data_args=data_args,\n",
    "    model_args=model_args,\n",
    "    training_args=training_args,\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    criterion=criterion,\n",
    "    aquisiiton_f=aquisition_function,\n",
    "    num_labels=num_labels,\n",
    "    bays_aversarial=bays_adv_approach,\n",
    "    iters=active_learning_iters)\n",
    "\n",
    "# run active learning experiment\n",
    "active_model.active_train(\n",
    "    initial_n_train=initial_train_data_size,\n",
    "    n_sample_infer=n_sample_size_pool_inference,\n",
    "    T_aq=T_aq_uncert,\n",
    "    n_query=n_query_pick,\n",
    "    aquisition_strategy=strategey,\n",
    "    use_swag=use_swag,\n",
    "    r_s=trial,\n",
    "    initial_train_seed=initial_train_seed,\n",
    "    mc_dropout=mc_dropout,\n",
    "    train_mode=train_mode\n",
    ")\n",
    "\n",
    "\n",
    "# trial performance\n",
    "ac_performace_temp = active_model.test_performance_ac\n",
    "b_ac_performace_temp = active_model.test_performance_b_ac\n",
    "f1_performance = active_model.test_performance_f1\n",
    "\n",
    "\n",
    "\n",
    "rrrr = active_model.data_handler.all_random_ccc\n",
    "\n",
    "assl_trinals_randoms[trial].append(rrrr)\n",
    "\n",
    "# number of train data\n",
    "number_of_train_data = active_model.number_of_train_data\n",
    "\n",
    "# adding metrics to matrix for averagind\n",
    "all_trials_ac_performace[trial,:] = ac_performace_temp\n",
    "all_trials_b_ac_performace[trial,:] = b_ac_performace_temp\n",
    "all_trials_f1_performace[trial,:] = f1_performance\n",
    "\n",
    "n_done += 1\n",
    "\n",
    "print(f'EXPERIMENT PROGRES: {round(n_done/num_experiemnts*100)}% done ')\n",
    "print(f'Experiment detials: \\n trials_strategy: {trials_strategy} \\n n_epochs_train: {n_epochs_train} \\n T_aq_uncert: {T_aq_uncert} \\n n_query_pick: {n_query_pick} \\n bays_adv_approach: {bays_adv_approach}\\n active_learning_iters: {active_learning_iters}\\n initial_train_data_size: {initial_train_data_size}\\n n_sample_size_pool_inference: {n_sample_size_pool_inference}\\n use_swag: {use_swag}\\n use_clustering: {use_clustering}\\n strategey: {strategey} ')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# powerpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls ../../../processed_pubmed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# dfff = pd.read_csv('../../../processed_pubmed/train.csv')\n",
    "# ' '.join([i for i in dfff[dfff.un_id ==dfff.un_id.unique()[0]].seqs.values])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dfff.un_id.unique()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls available_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dfff = pd.read_csv('./available_datasets/ag_news/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# c = 0\n",
    "\n",
    "# for i in dfff.label.unique():\n",
    "    \n",
    "#     temp_df = dfff[dfff.label == i].copy()\n",
    "    \n",
    "#     print(temp_df.text.values[0])\n",
    "#     print()\n",
    "    \n",
    "#     c += 1\n",
    "    \n",
    "#     if c == 3:\n",
    "        \n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "length_show = len(temp_df.text.values[0].split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len_per_line = 6\n",
    "\n",
    "s = 0\n",
    "end = 0\n",
    "end += len_per_line\n",
    "\n",
    "for i in range(15):\n",
    "    print(' '.join(temp_df.text.values[0].split(' ')[s:end]))\n",
    "    \n",
    "    s += len_per_line\n",
    "    end += len_per_line\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " 1 represents , 2 represents Sports, 3 represents Business and 4 represents Sci/Tech."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "['World','Sports','Business','Sci/Tech']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:packclone]",
   "language": "python",
   "name": "conda-env-packclone-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "286.688px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
